{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake Friends Data\n",
    "\n",
    "\n",
    "## Overview\n",
    "This notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. DBFS is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n",
    "\n",
    "This notebook is written in Python so the default cell type is Python. However, you can use different languages by using the %LANGUAGE syntax. Python, Scala, SQL, and R are all supported.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 20:01:54 WARN Utils: Your hostname, MBP-SMITH515.local resolves to a loopback address: 127.0.0.1; using 192.168.4.81 instead (on interface en0)\n",
      "23/04/09 20:01:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 20:01:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/09 20:01:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/04/09 20:01:55 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "Spark Session WebUI Port: 4042\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession;\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[4]\").appName(\"ISM6562 Spark App01\").enableHiveSupport().getOrCreate();\n",
    "\n",
    "# Let's get the SparkContext object. It's the entry point to the Spark API. It's created when you create a sparksession\n",
    "sc = spark.sparkContext  \n",
    "\n",
    "# note: If you have multiple spark sessions running (like from a previous notebook you've run), \n",
    "# this spark session webUI will be on a different port than the default (4040). One way to \n",
    "# identify this part is with the following line. If there was only one spark session running, \n",
    "# this will be 4040. If it's higher, it means there are still other spark sesssions still running.\n",
    "spark_session_port = spark.sparkContext.uiWebUrl.split(\":\")[-1]\n",
    "print(\"Spark Session WebUI Port: \" + spark_session_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will set the log level to ERROR. This will hide the INFO or WARNING messages that are printed out by default. If you want to see them, set this to INFO or WARN.\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.4.81:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ISM6562 Spark App01</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1083a4580>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-----------+\n",
      "| id|    name|age|friendcount|\n",
      "+---+--------+---+-----------+\n",
      "|  1|Jean-Luc| 26|          2|\n",
      "|  2|    Hugh| 55|        221|\n",
      "|  3|  Deanna| 40|        465|\n",
      "|  4|   Quark| 68|         21|\n",
      "|  5|  Weyoun| 59|        318|\n",
      "+---+--------+---+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "# see here for more info on the schema: https://spark.apache.org/docs/latest/sql-programming-guide.html#inferring-the-schema-using-reflection\n",
    "# and here https://sparkbyexamples.com/pyspark/pyspark-sql-types-datatype-with-examples/\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"friendcount\", IntegerType(), True),\n",
    "    ])\n",
    "\n",
    "friends = spark.read.csv('data/fakefriends.csv', header=False, schema=schema)\n",
    "\n",
    "# display the first 5 rows of the dataframe\n",
    "friends.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends.createOrReplaceTempView(\"fakefriends_csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running the cell below causes an error, you need to install the pyspark-magic module. Open a terminal (and make sure you have the spark conda environment active) and run the following command:\n",
    "```pip install sparksql-magic```\n",
    "\n",
    "NOTE: Remember, to activate the spark environment, run the following command:\n",
    "```conda activate spark```\n",
    "\n",
    "**NOTE2: You will need to restart your jupyter kernel after installing the module!!!!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sparksql_magic extension is already loaded. To reload it, use:\n",
      "  %reload_ext sparksql_magic\n"
     ]
    }
   ],
   "source": [
    "%load_ext sparksql_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only showing top 20 row(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">id</td><td style=\"font-weight: bold\">name</td><td style=\"font-weight: bold\">age</td><td style=\"font-weight: bold\">friendcount</td></tr><tr><td>1</td><td>Jean-Luc</td><td>26</td><td>2</td></tr><tr><td>2</td><td>Hugh</td><td>55</td><td>221</td></tr><tr><td>3</td><td>Deanna</td><td>40</td><td>465</td></tr><tr><td>4</td><td>Quark</td><td>68</td><td>21</td></tr><tr><td>5</td><td>Weyoun</td><td>59</td><td>318</td></tr><tr><td>6</td><td>Gowron</td><td>37</td><td>220</td></tr><tr><td>7</td><td>Will</td><td>54</td><td>307</td></tr><tr><td>8</td><td>Jadzia</td><td>38</td><td>380</td></tr><tr><td>9</td><td>Hugh</td><td>27</td><td>181</td></tr><tr><td>10</td><td>Odo</td><td>53</td><td>191</td></tr><tr><td>11</td><td>Ben</td><td>57</td><td>372</td></tr><tr><td>12</td><td>Keiko</td><td>54</td><td>253</td></tr><tr><td>13</td><td>Jean-Luc</td><td>56</td><td>444</td></tr><tr><td>14</td><td>Hugh</td><td>43</td><td>49</td></tr><tr><td>15</td><td>Rom</td><td>36</td><td>49</td></tr><tr><td>16</td><td>Weyoun</td><td>22</td><td>323</td></tr><tr><td>17</td><td>Odo</td><td>35</td><td>13</td></tr><tr><td>18</td><td>Jean-Luc</td><td>45</td><td>455</td></tr><tr><td>19</td><td>Geordi</td><td>60</td><td>246</td></tr><tr><td>20</td><td>Odo</td><td>67</td><td>220</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from fakefriends_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only showing top 20 row(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">age</td><td style=\"font-weight: bold\">AvgFriendCount</td></tr><tr><td>31</td><td>267.3</td></tr><tr><td>65</td><td>298.2</td></tr><tr><td>53</td><td>222.9</td></tr><tr><td>34</td><td>245.5</td></tr><tr><td>28</td><td>209.1</td></tr><tr><td>26</td><td>242.1</td></tr><tr><td>27</td><td>228.1</td></tr><tr><td>44</td><td>282.2</td></tr><tr><td>22</td><td>206.4</td></tr><tr><td>47</td><td>233.2</td></tr><tr><td>52</td><td>340.6</td></tr><tr><td>40</td><td>250.8</td></tr><tr><td>20</td><td>165.0</td></tr><tr><td>57</td><td>258.8</td></tr><tr><td>54</td><td>278.1</td></tr><tr><td>48</td><td>281.4</td></tr><tr><td>19</td><td>213.3</td></tr><tr><td>64</td><td>281.3</td></tr><tr><td>41</td><td>268.6</td></tr><tr><td>43</td><td>230.6</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql \n",
    "select age, round(avg(friendcount), 1) As AvgFriendCount from fakefriends_csv group by age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# since our friends data is only stored in volatile memory, let's save the table into our spark-warehouse\n",
    "friends.write.saveAsTable(\"fake_friends\", mode='overwrite')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
