{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing PySpark\n",
    "\n",
    "To begin, it is often good practice to create a conda environment for a project. These environments have their own installed set of packages; therefore, you can keep the 'base' environment as a 'clean' environment that has only the basics, and then create seperate environments for any major projects (or type of work). \n",
    "\n",
    "Each environment can also have its own python version.\n",
    "\n",
    "Let's begin by creating a new conda environment called pyspark that has python version 3.10 (note: 3.11 was recently released, but the current version of pyspark will not run in version 3.11 (as of March, 2023)). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Java 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark requires java 8, so we need to install the java runtime environment first. Follow the instructions to download and install java from https://www.java.com/en/download/\n",
    "\n",
    "NOTE: If you are using a M1 or M2 mac, you will need to take one of two approaches: \n",
    "1. Install the Rosetta 2 emulator (https://support.apple.com/en-us/HT211861) and then install java 8.\n",
    "2. RECOMMENDED: Install Azul Zulu JDK 8 (https://www.azul.com/downloads/zulu-community/?version=java-8-lts&os=macos&architecture=arm-64-bit&package=jdk) and then install java 8.\n",
    "   1. Make sure you set your JAVA HOME environment variable to the location of the JDK (e.g. /Library/Java/JavaVirtualMachines/zulu-8.jdk/Contents/Home)\n",
    "      * edit .zshrc and add the following lines\n",
    "        * ```export JAVA_HOME=/Library/Java/JavaVirtualMachines/zulu-8.jdk/Contents/Home```\n",
    "        * ```export PATH=$JAVA_HOME/bin:$PATH```\n",
    "\n",
    "For Windows or Mac's with x86 processors, you can install java 8 from https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html\n",
    "\n",
    "Make sure to test that you have java 8 installed by running the following command in your terminal:\n",
    "\n",
    "    java -version\n",
    "\n",
    "If you get an error or a difference version, you may need to alter/update your environment variables.\n",
    "* On windows, you can access your environment variables by searching for 'environment variables' in the start menu.\n",
    "  * Also, see here https://docs.oracle.com/en/database/oracle/machine-learning/oml4r/1.5.1/oread/creating-and-modifying-environment-variables-on-windows.html\n",
    "* On MacOS X86 processors, you can access your environment variables using the termoinal.\n",
    "  * https://phoenixnap.com/kb/set-environment-variable-mac\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a conda environment for your spark coding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```conda create --name spark python=3.10```\n",
    "\n",
    "We then need to activate the environment. This will change the terminal prompt to show the name of the environment.\n",
    "\n",
    "```conda activate spark```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install pyspark and findspark\n",
    "\n",
    "Now we can install pyspark and findspark. We will also install jupyter notebook, which is a great tool for running and sharing code. \n",
    "\n",
    "```conda install pyspark findspark jupyterlab```\n",
    "\n",
    "To launch a jupyter notebook, we can simply type ```jupyter notebook``` in the terminal. This notebook will use the 'spark' conda environment that is currently active. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Creating a pyspark jupyter kernal\n",
    "\n",
    "If you wish to have this environment selectable within jupyter lab, you can install the conda environment as a kernel. \n",
    "\n",
    "```python -m ipykernel install --user --name=spark --display-name=\"Python (spark)\"```\n",
    "\n",
    "* NOTE: To verify is the kernel is installed, you can run ```jupyter kernelspec list``` in the terminal. This will list all of the kernels that are installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install other packages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new 'spark' environment has only the basics installed, so we will need to install the packages we need.\n",
    "\n",
    "For now, we will need to install pandas, matplotlib, jupyter lab, and pyspark (you may need to install more packages later; but you should know how to do this by now)\n",
    "\n",
    "```conda install -c conda-forge pandas=1.5.3 matplotlib sparkmagic```\n",
    "\n",
    "#### NOTE: Pandas is transitioning to PyArrrow - at the time of this writing, Pandas 2.0 is released and the version tha Conda installs. However, PySpark does not yet seem to support pandas with PyArrow, so we need to install Pandas 1.5.3. Look for this to change soon."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing you PySpark installation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to create and use a spark session. Make sure you download the BostonHousing.csv dataset (available in canvas and in the class github repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 16:16:08 WARN Utils: Your hostname, MBP-SMITH515.local resolves to a loopback address: 127.0.0.1; using 192.168.4.81 instead (on interface en0)\n",
      "23/04/09 16:16:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 16:16:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.4.81:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ISM6562 Spark App01</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10a607e80>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession;\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[4]\").appName(\"ISM6562 Spark App01\").enableHiveSupport().getOrCreate();\n",
    "\n",
    "# note: If you have multiple spark sessions running (like from a previous notebook you've run), \n",
    "# this spark session webUI will be on a different port than the default (4040). One way to \n",
    "# identify this part is with the following line. If there was only one spark session running, \n",
    "# this will be 4040. If it's higher, it means there are still other spark sesssions still running.\n",
    "spark_session_port = spark.sparkContext.uiWebUrl.split(\":\")[-1]\n",
    "print(\"Spark Session WebUI Port: \" + spark_session_port)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the link \"Spark UI\" that is displayed after running the code above. If everything is working, you should see that your spark session is running.\n",
    "\n",
    "*NOTE: Keep in mind that when you're running a notebook, it's not just the code in the notebook that's running. Until you hit the shutdown or restart kernal button, the spark session is still running. So if you run the above code, you'll see that the port is 4041. If you run the code again, it will be 4042. If you run the code again, it will be 4043. And so on.*\n",
    "\n",
    "If you need stop the spark session you can therefore restart/stop the kernal or run the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
