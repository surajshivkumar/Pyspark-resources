{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/10 07:02:42 WARN Utils: Your hostname, MBP-SMITH515.local resolves to a loopback address: 127.0.0.1; using 192.168.4.81 instead (on interface en0)\n",
      "23/04/10 07:02:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/10 07:02:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Spark Session WebUI Port: 4040\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession;\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[4]\").appName(\"ISM6562 Spark App01\").enableHiveSupport().getOrCreate();\n",
    "\n",
    "# Let's get the SparkContext object. It's the entry point to the Spark API. It's created when you create a sparksession\n",
    "sc = spark.sparkContext  \n",
    "\n",
    "# note: If you have multiple spark sessions running (like from a previous notebook you've run), \n",
    "# this spark session webUI will be on a different port than the default (4040). One way to \n",
    "# identify this part is with the following line. If there was only one spark session running, \n",
    "# this will be 4040. If it's higher, it means there are still other spark sesssions still running.\n",
    "spark_session_port = spark.sparkContext.uiWebUrl.split(\":\")[-1]\n",
    "print(\"Spark Session WebUI Port: \" + spark_session_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this will set the log level to ERROR. This will hide the INFO or WARNING messages that are printed out by default. If you want to see them, set this to INFO or WARN.\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.4.81:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ISM6562 Spark App01</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x107aa9180>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------+------+--------------------+\n",
      "|movieid|            title|       date|unkown|                 url|\n",
      "+-------+-----------------+-----------+------+--------------------+\n",
      "|      1| Toy Story (1995)|01-Jan-1995|  null|http://us.imdb.co...|\n",
      "|      2| GoldenEye (1995)|01-Jan-1995|  null|http://us.imdb.co...|\n",
      "|      3|Four Rooms (1995)|01-Jan-1995|  null|http://us.imdb.co...|\n",
      "|      4|Get Shorty (1995)|01-Jan-1995|  null|http://us.imdb.co...|\n",
      "|      5|   Copycat (1995)|01-Jan-1995|  null|http://us.imdb.co...|\n",
      "+-------+-----------------+-----------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "# see here for more info on the schema: https://spark.apache.org/docs/latest/sql-programming-guide.html#inferring-the-schema-using-reflection\n",
    "# and here https://sparkbyexamples.com/pyspark/pyspark-sql-types-datatype-with-examples/\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"movieid\", IntegerType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"unkown\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "movies = spark.read.csv('data/u.item', header=False, schema=schema,  sep = '|')\n",
    "\n",
    "# display the first 5 rows of the dataframe\n",
    "movies.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "movies.createOrReplaceTempView(\"movies_tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext sparksql_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# demonstration, note that when using sparksql, we can save the results in a temporary view\n",
    "# but this (and other sparksql switches) will not work with VSCode. It will work in Jupyter Notebook.\n",
    "# %%sparksql --view tempdf\n",
    "# select * from movies_tmp limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can use sparksql to show current tables, but this will only work in Jupyter Notebook. It will \n",
    "# not work in VSCode.\n",
    "#%%sparksql \n",
    "#SHOW TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|movieid|               title|\n",
      "+-------+--------------------+\n",
      "|      1|    Toy Story (1995)|\n",
      "|      2|    GoldenEye (1995)|\n",
      "|      3|   Four Rooms (1995)|\n",
      "|      4|   Get Shorty (1995)|\n",
      "|      5|      Copycat (1995)|\n",
      "|      6|Shanghai Triad (Y...|\n",
      "|      7|Twelve Monkeys (1...|\n",
      "|      8|         Babe (1995)|\n",
      "|      9|Dead Man Walking ...|\n",
      "|     10|  Richard III (1995)|\n",
      "|     11|Seven (Se7en) (1995)|\n",
      "|     12|Usual Suspects, T...|\n",
      "|     13|Mighty Aphrodite ...|\n",
      "|     14|  Postino, Il (1994)|\n",
      "|     15|Mr. Holland's Opu...|\n",
      "|     16|French Twist (Gaz...|\n",
      "|     17|From Dusk Till Da...|\n",
      "|     18|White Balloon, Th...|\n",
      "|     19|Antonia's Line (1...|\n",
      "|     20|Angels and Insect...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df = spark.sql(\"\"\"SELECT\n",
    "  movieid,\n",
    "  title\n",
    "FROM movies_tmp\"\"\"\n",
    ")\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.write.saveAsTable(\"movies\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userid|movieid|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|   196|    242|     3|881250949|\n",
      "|   186|    302|     3|891717742|\n",
      "|    22|    377|     1|878887116|\n",
      "|   244|     51|     2|880606923|\n",
      "|   166|    346|     1|886397596|\n",
      "+------+-------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "# see here for more info on the schema: https://spark.apache.org/docs/latest/sql-programming-guide.html#inferring-the-schema-using-reflection\n",
    "# and here https://sparkbyexamples.com/pyspark/pyspark-sql-types-datatype-with-examples/\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"userid\", IntegerType(), True),\n",
    "    StructField(\"movieid\", IntegerType(), True),\n",
    "    StructField(\"rating\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "movierating = spark.read.csv('data/u.data', header=False, schema=schema,  sep = '\\t')\n",
    "\n",
    "# display the first 5 rows of the dataframe\n",
    "movierating.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "movierating.write.saveAsTable(\"movieratings\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">userid</td><td style=\"font-weight: bold\">movieid</td><td style=\"font-weight: bold\">rating</td><td style=\"font-weight: bold\">timestamp</td></tr><tr><td>196</td><td>242</td><td>3</td><td>881250949</td></tr><tr><td>186</td><td>302</td><td>3</td><td>891717742</td></tr><tr><td>22</td><td>377</td><td>1</td><td>878887116</td></tr><tr><td>244</td><td>51</td><td>2</td><td>880606923</td></tr><tr><td>166</td><td>346</td><td>1</td><td>886397596</td></tr><tr><td>298</td><td>474</td><td>4</td><td>884182806</td></tr><tr><td>115</td><td>265</td><td>2</td><td>881171488</td></tr><tr><td>253</td><td>465</td><td>5</td><td>891628467</td></tr><tr><td>305</td><td>451</td><td>3</td><td>886324817</td></tr><tr><td>6</td><td>86</td><td>3</td><td>883603013</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from movieratings limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRating = spark.table('movieratings')\n",
    "dfMovies = spark.table('movies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+----------+\n",
      "|userid|movieid|rating|timestamp|prediction|\n",
      "+------+-------+------+---------+----------+\n",
      "|   580|    471|     3|884125018| 3.0697474|\n",
      "|   588|    471|     5|890024289| 3.9763558|\n",
      "|   804|    496|     5|879441973|  4.184652|\n",
      "|   472|    496|     4|875980823| 3.8299975|\n",
      "|   633|    148|     1|875326138|  3.783264|\n",
      "|   300|    833|     4|875650329| 5.5967994|\n",
      "|   406|    496|     4|879445378| 3.6402574|\n",
      "|    26|    148|     3|891377540| 2.4295914|\n",
      "|    27|    148|     3|891543129|   2.75539|\n",
      "|   577|    496|     5|880474455|  4.420952|\n",
      "|    44|    148|     4|878346946| 3.8073215|\n",
      "|   159|    471|     4|880485861|  3.867787|\n",
      "|   806|    496|     5|882387798|  3.966246|\n",
      "|   103|    471|     4|880416921| 3.0895875|\n",
      "|   236|    148|     4|890117028|  2.495822|\n",
      "|   409|    496|     5|881107817| 3.7729216|\n",
      "|   601|    496|     4|876349302| 3.4476748|\n",
      "|   128|    471|     4|879967804| 3.3168216|\n",
      "|   330|    148|     4|876544781| 4.1150646|\n",
      "|   727|    471|     3|883709188| 3.4162269|\n",
      "+------+-------+------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for more on colaborative filtering, see here https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html\n",
    "# \n",
    "from pyspark.ml.recommendation import ALS\n",
    " \n",
    "#split training and testing\n",
    "(dftraining, dftest) = dfRating.randomSplit([0.8, 0.2])\n",
    " \n",
    "## Build the recommendation model using ALS on the training data\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userid\", \n",
    "    itemCol=\"movieid\", ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\")\n",
    "model = als.fit(dftraining)\n",
    " \n",
    "#display predicted rating\n",
    "predictions = model.transform(dftest)\n",
    "predictions.show()\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
